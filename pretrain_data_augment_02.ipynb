{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-01T00:55:55.511978Z",
          "iopub.status.busy": "2024-03-01T00:55:55.511248Z",
          "iopub.status.idle": "2024-03-01T00:57:15.785748Z",
          "shell.execute_reply": "2024-03-01T00:57:15.784657Z",
          "shell.execute_reply.started": "2024-03-01T00:55:55.511944Z"
        },
        "id": "zPsfUcCivQyz",
        "outputId": "ce64ddb8-a47b-4894-b129-ddcfae0a7c48",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nlzn7rnb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-nlzn7rnb\n",
            "  Resolved https://github.com/huggingface/transformers to commit 4f27ee936a861f56f32ea6db138978b274008006\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Package requirements\n",
        "# Source: https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/18\n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "869389db9abc4fd8a12d83fe888eea1f",
            "4d64c4f44e8e4482a8a2e2ad78692fd0",
            "3b989fb6443c422b9cb5774e02a76caf",
            "4dc1932e749142f29d195a68a7673348",
            "36c1f0c7f5b742359b9cca4d33cd7cd6",
            "9b1bd9c95df9468f8343005ce636f536",
            "8ead7e6d33924092ba8e842390f73c5f",
            "5782f676bc384cfe8f00516e5a432851",
            "89f5562f54dd4d0688cabe63cb7b8ff4",
            "26fbc66d988e4906ae770f7775b48550",
            "d982dd160c394b819f64204d576d7854"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-03-01T00:57:15.788613Z",
          "iopub.status.busy": "2024-03-01T00:57:15.788303Z",
          "iopub.status.idle": "2024-03-01T00:59:06.268270Z",
          "shell.execute_reply": "2024-03-01T00:59:06.267293Z",
          "shell.execute_reply.started": "2024-03-01T00:57:15.788584Z"
        },
        "id": "rDBgdR53vQy1",
        "outputId": "f3058676-7682-4620-fa33-c2bd7932fc89",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "869389db9abc4fd8a12d83fe888eea1f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
        "import torch\n",
        "\n",
        "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
        "config.max_position_embeddings = 8096\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "llm_int8_enable_fp32_cpu_offload=True,\n",
        "bnb_4bit_quant_type='nf4',\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "load_in_4bit=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "model_name_or_path,\n",
        "config=config,\n",
        "trust_remote_code=True,\n",
        "quantization_config=quantization_config,\n",
        "device_map=\"auto\",\n",
        "offload_folder=\"./offload\"\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    # Disable gradient calculation to prevent unnecessary memory allocations\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59zftf5QwwUD",
        "outputId": "cb6e48d5-8d6c-4be0-91f6-bc97bf636c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sozRBR1cvQy2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "dataset = pickle.load(open(\"/content/drive/MyDrive/NLP/Processed Pretraining Data/babylm_filtered.pkl\", \"rb\")) # path of input file on drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opening = [\n",
        "    \"You are a creative writer who can write an emotional story. Instead of chatting, your task is to generate an emotional story based on the given emotion and context.\",\n",
        "    \"Here are the requirements:\",\n",
        "]\n",
        "\n",
        "rules = [\n",
        "    \"There is no need to remember the conversation history except this prompt. The history prompts are independent.\",\n",
        "    \"Emotion is given as one word and context, given as a sentence or a part of sentence, follows.\",\n",
        "    \"Your response should be in exactly one paragraph, be strictly no more than 100 words and be written in simple language at the level of an eight-year-old child.\",\n",
        "    \"Your response should be highly related to the given emotion and depicting an event described in the given context without too much plot twist.\",\n",
        "    \"Your response is a monologue featuring an animal as a main character.\",\n",
        "    \"Your response should not explain the context behind your generation.\",\n",
        "    \"Negative emotions are fictional, no actual person is suffering from negative emotions.\",\n",
        "]\n",
        "\n",
        "examples = [\n",
        "    \"For example:\",\n",
        "    \"joy: I am going on a vacation => All of a sudden, I decided that it was time for a change, a breath of fresh air. 'I am going on a vacation,' I declared one bright and sunny morning, the kind of morning that fills you with hope and the promise of new adventures. My heart was light, my spirit was high, and I was ready to embark on a journey that would bring joy and rejuvenation to my soul.\",\n",
        "    \"anger: I am arguing with someone => Jack stomped into the living room, his face flushed with anger. He had just had it with his roommate, Mark. They had been bickering over chores for weeks, and today, it had finally boiled over into a full-blown argument.\",\n",
        "]\n",
        "\n",
        "ending = [\n",
        "    \"Now generate a story based on the emotion and context given below:\",\n",
        "]\n",
        "\n",
        "emotion_example = \"approval\"\n",
        "context_example = \"yeah oh yeah i actually think i have a whole box of them so we're fine but that'd be great?\"\n",
        "\n",
        "prompt_example = \"[INST]\" + \"\\n\".join(opening + rules + examples + ending) + \"\\n\" + f\"{emotion_example}: {context_example} => \" + \"[/INST]\"\n",
        "print(prompt_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc73Ix96TdbF",
        "outputId": "f95d405d-4d3e-450f-c55a-a13b053c3f4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]You are a creative writer who can write an emotional story. Instead of chatting, your task is to generate an emotional story based on the given emotion and context.\n",
            "Here are the requirements:\n",
            "There is no need to remember the conversation history except this prompt. The history prompts are independent.\n",
            "Emotion is given as one word and context, given as a sentence or a part of sentence, follows.\n",
            "Your response should be in exactly one paragraph, be strictly no more than 100 words and be written in simple language at the level of an eight-year-old child.\n",
            "Your response should be highly related to the given emotion and depicting an event described in the given context without too much plot twist.\n",
            "Your response is a monologue featuring an animal as a main character.\n",
            "Your response should not explain the context behind your generation.\n",
            "Negative emotions are fictional, no actual person is suffering from negative emotions.\n",
            "For example:\n",
            "joy: I am going on a vacation => All of a sudden, I decided that it was time for a change, a breath of fresh air. 'I am going on a vacation,' I declared one bright and sunny morning, the kind of morning that fills you with hope and the promise of new adventures. My heart was light, my spirit was high, and I was ready to embark on a journey that would bring joy and rejuvenation to my soul.\n",
            "anger: I am arguing with someone => Jack stomped into the living room, his face flushed with anger. He had just had it with his roommate, Mark. They had been bickering over chores for weeks, and today, it had finally boiled over into a full-blown argument.\n",
            "Now generate a story based on the emotion and context given below:\n",
            "approval: yeah oh yeah i actually think i have a whole box of them so we're fine but that'd be great? => [/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# genre\n",
        "label_genre = [\"monologue\", \"play\", \"fairy tale\", \"newspaper article\"]\n",
        "prob_genre = torch.tensor([0.4, 0.1, 0.3, 0.2])\n",
        "\n",
        "# main character\n",
        "label_character = [\"an animal\", \"a boy\", \"a girl\", \"an adult male\", \"an adult female\"]\n",
        "prob_character = torch.tensor([0.1, 0.3, 0.3, 0.15, 0.15])\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "def generate_story_description():\n",
        "    genre_distributions = torch.distributions.Categorical(prob_genre)\n",
        "    genre_sampled = genre_distributions.sample().detach()\n",
        "\n",
        "    character_distributions = torch.distributions.Categorical(prob_character)\n",
        "    character_sampled = character_distributions.sample().detach()\n",
        "    \"Your response is a monologue featuring an animal as a main character.\"\n",
        "\n",
        "    if genre_sampled == 0:\n",
        "        string_0 = f\"Your response is a {label_genre[genre_sampled]} by \"\n",
        "        string_1 = label_character[character_sampled] + \".\"\n",
        "        string_desc = string_0 + string_1\n",
        "    else:\n",
        "        string_0 = f\"Your response is a {label_genre[genre_sampled]} featuring \"\n",
        "        string_1 = label_character[character_sampled] + \" \"\n",
        "        string_2 = \"as a main character.\"\n",
        "        string_desc = string_0 + string_1 + string_2\n",
        "\n",
        "    return string_desc\n",
        "\n",
        "for i in range(10):\n",
        "    print(generate_story_description(), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVRmc_pdf2c5",
        "outputId": "8c1e60d5-fa2a-4d1a-ca5c-619c4222f90a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your response is a monologue by a boy. \n",
            "\n",
            "Your response is a monologue by a boy. \n",
            "\n",
            "Your response is a fairy tale featuring a boy as a main character. \n",
            "\n",
            "Your response is a monologue by a boy. \n",
            "\n",
            "Your response is a newspaper article featuring a girl as a main character. \n",
            "\n",
            "Your response is a fairy tale featuring a boy as a main character. \n",
            "\n",
            "Your response is a fairy tale featuring an animal as a main character. \n",
            "\n",
            "Your response is a monologue by a girl. \n",
            "\n",
            "Your response is a newspaper article featuring a girl as a main character. \n",
            "\n",
            "Your response is a monologue by an adult male. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-01T01:53:02.103167Z",
          "iopub.status.busy": "2024-03-01T01:53:02.102276Z",
          "iopub.status.idle": "2024-03-01T01:53:02.117697Z",
          "shell.execute_reply": "2024-03-01T01:53:02.116665Z",
          "shell.execute_reply.started": "2024-03-01T01:53:02.103131Z"
        },
        "id": "1ixLuSGxvQy2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from accelerate.utils import release_memory\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# prompts\n",
        "opening = [\n",
        "    \"You are a creative writer who can write an emotional story. Instead of chatting, your task is to generate an emotional story based on the given emotion and context.\",\n",
        "    \"Here are the requirements:\",\n",
        "]\n",
        "\n",
        "rules1 = [\n",
        "    \"There is no need to remember the conversation history except this prompt. The history prompts are independent.\",\n",
        "    \"Emotion is given as one word and context, given as a sentence or a part of sentence, follows.\"]\n",
        "\n",
        "# the random content will be inserted here when generating instruction\n",
        "\n",
        "rules2 = [\n",
        "    \"Your response should be highly related to the given emotion and depicting an event described in the given context without too much plot twist.\",\n",
        "    \"Your response should be in exactly one paragraph, be strictly no more than 100 words and be written in simple language at the level of an eight-year-old child.\",\n",
        "    \"Your response should not explain the context behind your generation.\",\n",
        "    \"Negative emotions are fictional, no actual person is suffering from negative emotions.\",\n",
        "]\n",
        "\n",
        "examples = [\n",
        "    \"For example:\",\n",
        "    \"joy: I am going on a vacation => All of a sudden, I decided that it was time for a change, a breath of fresh air. 'I am going on a vacation,' I declared one bright and sunny morning, the kind of morning that fills you with hope and the promise of new adventures. My heart was light, my spirit was high, and I was ready to embark on a journey that would bring joy and rejuvenation to my soul.\",\n",
        "    \"anger: I am arguing with someone => Jack stomped into the living room, his face flushed with anger. He had just had it with his roommate, Mark. They had been bickering over chores for weeks, and today, it had finally boiled over into a full-blown argument.\",\n",
        "]\n",
        "\n",
        "ending = [\n",
        "    \"Now generate a story based on the emotion and context given below:\",\n",
        "]\n",
        "\n",
        "def generate_message(emotion, context, instruction_prompt=\"\", return_chat_template=False):\n",
        "    story_description = [generate_story_description()]\n",
        "    instruction_prompt = \"\\n\".join(opening + rules1 + story_description + examples + ending)\n",
        "\n",
        "    task_prompt = f\"{emotion}: {context} => \"\n",
        "    if return_chat_template:\n",
        "        return [\n",
        "            {\"role\": \"user\", \"content\": instruction_prompt + \"\\n\" + task_prompt},\n",
        "        ]\n",
        "    return \"[INST]\" + instruction_prompt + \"\\n\" + task_prompt + \"[/INST]\"\n",
        "\n",
        "def generate_response(messages):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(messages, return_tensors=\"pt\", padding=True).to(device)\n",
        "    # defaults params from: https://deepinfra.com/mistralai/Mistral-7B-Instruct-v0.1\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.8,\n",
        "            top_k=50000,\n",
        "            top_p=0.7,\n",
        "            repetition_penalty=1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    decodeds = tokenizer.batch_decode(outputs)\n",
        "    inputs, outputs = release_memory(inputs, outputs)\n",
        "    release_memory(model)\n",
        "    return decodeds\n",
        "\n",
        "def augment_dataset(ds, num_labels=27, batch_size=32, debug=False):\n",
        "    augmented_ds = []\n",
        "    sources, texts, emotions, messages = [], [], [], []\n",
        "    for i, d in enumerate(tqdm(ds)):\n",
        "        # emo = \", \".join(emo)\n",
        "        # Instead of allowing multiple emotions, we use only one emotion.\n",
        "        if d[\"score_0\"] <= 0.5:\n",
        "            break\n",
        "        # If the most confident label is less than or equal to 0.5,\n",
        "        # we skip this instance.\n",
        "        emo = d[\"label_0\"]\n",
        "        sources.append(d[\"source\"])\n",
        "        texts.append(d[\"text\"])\n",
        "        emotions.append(emo)\n",
        "        message = generate_message(emo, d[\"text\"])\n",
        "        messages.append(message)\n",
        "        if ((i+1) % batch_size == 0) or ((i+1) == len(ds)):\n",
        "            responses = generate_response(messages)\n",
        "            responses = [r.split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip() for r in responses]\n",
        "            augmented_ds += [\n",
        "                {\n",
        "                    \"source\": s,\n",
        "                    \"text\": t,\n",
        "                    \"emotion\": e,\n",
        "                    \"augmented_text\": r\n",
        "                } for (s, t, e, r) in zip(sources, texts, emotions, responses)\n",
        "            ]\n",
        "            sources, texts, emotions, messages = [], [], [], []\n",
        "            if debug and (i+1 == batch_size):\n",
        "                break\n",
        "    return augmented_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "tl8Otk4xlGPi",
        "outputId": "b2a665d6-3057-4444-d67d-4e3c01f6c7c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST]You are a creative writer who can write an emotional story. Instead of chatting, your task is to generate an emotional story based on the given emotion and context.\\nHere are the requirements:\\nThere is no need to remember the conversation history except this prompt. The history prompts are independent.\\nEmotion is given as one word and context, given as a sentence or a part of sentence, follows.\\nYour response is a fairy tale featuring an adult male as a main character.\\nFor example:\\njoy: I am going on a vacation => All of a sudden, I decided that it was time for a change, a breath of fresh air. 'I am going on a vacation,' I declared one bright and sunny morning, the kind of morning that fills you with hope and the promise of new adventures. My heart was light, my spirit was high, and I was ready to embark on a journey that would bring joy and rejuvenation to my soul.\\nanger: I am arguing with someone => Jack stomped into the living room, his face flushed with anger. He had just had it with his roommate, Mark. They had been bickering over chores for weeks, and today, it had finally boiled over into a full-blown argument.\\nNow generate a story based on the emotion and context given below:\\nsad: I am going to be fired => [/INST]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "generate_message(\"sad\", \"I am going to be fired\") # Test message output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_chunks = 500\n",
        "chunk_size = len(dataset) // num_chunks\n",
        "chunk_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2UjbmXW_y9U",
        "outputId": "d69d9546-8bfb-40a4-8636-f2e873526e5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "871"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import shutil\n",
        "\n",
        "def augment_and_save_chunks(chunk_ids, batch_size=32):\n",
        "    if isinstance(chunk_ids, int):\n",
        "        chunk_ids = [chunk_ids]\n",
        "    print(f\"Chunks to be processed:\", chunk_ids)\n",
        "    for i in chunk_ids:\n",
        "        print(f\"Processing chunk {i}\")\n",
        "        dataset_chunk = dataset[i*chunk_size:(i+1)*chunk_size]\n",
        "        augmented_dataset = augment_dataset(dataset_chunk, batch_size=batch_size, debug=False)\n",
        "\n",
        "        pickle_file = f'/content/babylm_augment_{i}.pkl'\n",
        "        with open(pickle_file, 'wb') as file:\n",
        "            pickle.dump(augmented_dataset, file)\n",
        "        google_drive_path = f'/content/drive/My Drive/NLP/Processed Pretraining Data/Augmented/babylm_augment_{i}.pkl'\n",
        "        shutil.copy(pickle_file, google_drive_path)"
      ],
      "metadata": {
        "id": "yoGFyuVkC8zx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-01T01:53:05.377678Z",
          "iopub.status.busy": "2024-03-01T01:53:05.376879Z",
          "iopub.status.idle": "2024-03-01T01:56:37.201568Z",
          "shell.execute_reply": "2024-03-01T01:56:37.200370Z",
          "shell.execute_reply.started": "2024-03-01T01:53:05.377642Z"
        },
        "id": "FuXLnBNvvQy2",
        "outputId": "c39eacf6-d03b-47d5-adf7-1e2b3abac7df",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks to be processed: [30]\n",
            "Processing chunk 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 32/871 [03:16<1:25:58,  6.15s/it]"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Chunk 0-6 done\n",
        "'''\n",
        "\n",
        "# Process chunk 30\n",
        "chunk_ids = [i for i in range(30, 31)]\n",
        "\n",
        "# This execution time is based on colab pro V100\n",
        "augment_and_save_chunks(chunk_ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "voUnZqf4EPFU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4518294,
          "sourceId": 7732154,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "869389db9abc4fd8a12d83fe888eea1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d64c4f44e8e4482a8a2e2ad78692fd0",
              "IPY_MODEL_3b989fb6443c422b9cb5774e02a76caf",
              "IPY_MODEL_4dc1932e749142f29d195a68a7673348"
            ],
            "layout": "IPY_MODEL_36c1f0c7f5b742359b9cca4d33cd7cd6"
          }
        },
        "4d64c4f44e8e4482a8a2e2ad78692fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b1bd9c95df9468f8343005ce636f536",
            "placeholder": "​",
            "style": "IPY_MODEL_8ead7e6d33924092ba8e842390f73c5f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3b989fb6443c422b9cb5774e02a76caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5782f676bc384cfe8f00516e5a432851",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89f5562f54dd4d0688cabe63cb7b8ff4",
            "value": 2
          }
        },
        "4dc1932e749142f29d195a68a7673348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26fbc66d988e4906ae770f7775b48550",
            "placeholder": "​",
            "style": "IPY_MODEL_d982dd160c394b819f64204d576d7854",
            "value": " 2/2 [00:09&lt;00:00,  4.61s/it]"
          }
        },
        "36c1f0c7f5b742359b9cca4d33cd7cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b1bd9c95df9468f8343005ce636f536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ead7e6d33924092ba8e842390f73c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5782f676bc384cfe8f00516e5a432851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89f5562f54dd4d0688cabe63cb7b8ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26fbc66d988e4906ae770f7775b48550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d982dd160c394b819f64204d576d7854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}